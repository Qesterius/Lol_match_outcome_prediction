{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reading timestamps to filter out ones we are interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "timestamps_data = pd.read_csv(\"data_csv/timestamps_ext.csv\")\n",
    "target = pd.read_csv(\"data_csv/first_win.csv\")\n",
    "target.columns = [\"matchId\", \"first_win\"]\n",
    "data = timestamps_data.merge(target, on=\"matchId\")\n",
    "del target\n",
    "del timestamps_data\n",
    "\n",
    "data_cleaned = data.dropna()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps_starting = data_cleaned[data_cleaned[\"timeStamp\"] == 0]\n",
    "timestamps_20p = data_cleaned.loc[\n",
    "    data_cleaned.groupby(\"matchId\")[\"percentTimeStamp\"].apply(\n",
    "        lambda x: (x - 0.2).abs().idxmin()\n",
    "    )\n",
    "]\n",
    "timestamps_40p = data_cleaned.loc[\n",
    "    data_cleaned.groupby(\"matchId\")[\"percentTimeStamp\"].apply(\n",
    "        lambda x: (x - 0.4).abs().idxmin()\n",
    "    )\n",
    "]\n",
    "timestamps_60p = data_cleaned.loc[\n",
    "    data_cleaned.groupby(\"matchId\")[\"percentTimeStamp\"].apply(\n",
    "        lambda x: (x - 0.6).abs().idxmin()\n",
    "    )\n",
    "]\n",
    "timestamps_80p = data_cleaned.loc[\n",
    "    data_cleaned.groupby(\"matchId\")[\"percentTimeStamp\"].apply(\n",
    "        lambda x: (x - 0.8).abs().idxmin()\n",
    "    )\n",
    "]\n",
    "timestamps_100p = data_cleaned.loc[\n",
    "    data_cleaned.groupby(\"matchId\")[\"percentTimeStamp\"].apply(\n",
    "        lambda x: (x - 1.0).abs().idxmin()\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make data_cleaned only include whats in timestamps_starting .. 100\n",
    "\n",
    "data_cleaned = pd.concat(\n",
    "    [timestamps_starting, timestamps_20p, timestamps_40p, timestamps_60p, timestamps_80p, timestamps_100p]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading matches data to filtered timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1732\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "588"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del data\n",
    "matches_data = pd.read_csv(\"data_csv/matches.csv\")\n",
    "data_cleaned = data_cleaned.merge(matches_data, on=\"matchId\")\n",
    "print(len(data_cleaned.columns))\n",
    "del matches_data\n",
    "#call garbagecollector \n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if matches.csv is having player ids in columns 0-9 instead of 1-10 uncomment below and paste as first line whats in output into matches.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"data_csv/matches.csv\", \"r\") as f:\n",
    "#     columns = f.readline().strip().split(\",\")\n",
    "\n",
    "# #replace each number in columns with number+1\n",
    "# import regex as re\n",
    "# regex = re.compile(r\"p(\\d+)_\")\n",
    "\n",
    "# columns = [regex.sub(lambda x: f\"p{int(x.group(1))+1}_\", col) for col in columns]\n",
    "\n",
    "# print(\",\".join(columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.63 GiB for an array with shape (1510, 145302) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m data_cleaned\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtournamentCode\u001b[39m\u001b[38;5;124m\"\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     16\u001b[0m data_cleaned\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_puuid\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m11\u001b[39m)], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 17\u001b[0m \u001b[43mdata_cleaned\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m(\u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Users\\qesterius\\Documents\\Studia\\sztuczna-inteligencja-w-systemach-inf\\.venv\\lib\\site-packages\\pandas\\core\\frame.py:6673\u001b[0m, in \u001b[0;36mDataFrame.dropna\u001b[1;34m(self, axis, how, thresh, subset, inplace, ignore_index)\u001b[0m\n\u001b[0;32m   6670\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvalid how option: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhow\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6672\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(mask):\n\u001b[1;32m-> 6673\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   6674\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6675\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc(axis\u001b[38;5;241m=\u001b[39maxis)[mask]\n",
      "File \u001b[1;32me:\\Users\\qesterius\\Documents\\Studia\\sztuczna-inteligencja-w-systemach-inf\\.venv\\lib\\site-packages\\pandas\\core\\generic.py:6808\u001b[0m, in \u001b[0;36mNDFrame.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m   6659\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   6660\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcopy\u001b[39m(\u001b[38;5;28mself\u001b[39m, deep: bool_t \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m   6661\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   6662\u001b[0m \u001b[38;5;124;03m    Make a copy of this object's indices and data.\u001b[39;00m\n\u001b[0;32m   6663\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   6806\u001b[0m \u001b[38;5;124;03m    dtype: int64\u001b[39;00m\n\u001b[0;32m   6807\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 6808\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n\u001b[0;32m   6810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(data, axes\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39maxes)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[0;32m   6811\u001b[0m         \u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcopy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   6812\u001b[0m     )\n",
      "File \u001b[1;32me:\\Users\\qesterius\\Documents\\Studia\\sztuczna-inteligencja-w-systemach-inf\\.venv\\lib\\site-packages\\pandas\\core\\internals\\managers.py:604\u001b[0m, in \u001b[0;36mBaseBlockManager.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    601\u001b[0m         res\u001b[38;5;241m.\u001b[39m_blklocs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blklocs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[1;32m--> 604\u001b[0m     \u001b[43mres\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_consolidate_inplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32me:\\Users\\qesterius\\Documents\\Studia\\sztuczna-inteligencja-w-systemach-inf\\.venv\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1786\u001b[0m, in \u001b[0;36mBlockManager._consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_consolidate_inplace\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1781\u001b[0m     \u001b[38;5;66;03m# In general, _consolidate_inplace should only be called via\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m     \u001b[38;5;66;03m#  DataFrame._consolidate_inplace, otherwise we will fail to invalidate\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m     \u001b[38;5;66;03m#  the DataFrame's _item_cache. The exception is for newly-created\u001b[39;00m\n\u001b[0;32m   1784\u001b[0m     \u001b[38;5;66;03m#  BlockManager objects not yet attached to a DataFrame.\u001b[39;00m\n\u001b[0;32m   1785\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_consolidated():\n\u001b[1;32m-> 1786\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m \u001b[43m_consolidate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1787\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1788\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32me:\\Users\\qesterius\\Documents\\Studia\\sztuczna-inteligencja-w-systemach-inf\\.venv\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2267\u001b[0m, in \u001b[0;36m_consolidate\u001b[1;34m(blocks)\u001b[0m\n\u001b[0;32m   2265\u001b[0m new_blocks: \u001b[38;5;28mlist\u001b[39m[Block] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   2266\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (_can_consolidate, dtype), group_blocks \u001b[38;5;129;01min\u001b[39;00m grouper:\n\u001b[1;32m-> 2267\u001b[0m     merged_blocks, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_merge_blocks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2268\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgroup_blocks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcan_consolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_can_consolidate\u001b[49m\n\u001b[0;32m   2269\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2270\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(merged_blocks, new_blocks)\n\u001b[0;32m   2271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(new_blocks)\n",
      "File \u001b[1;32me:\\Users\\qesterius\\Documents\\Studia\\sztuczna-inteligencja-w-systemach-inf\\.venv\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2299\u001b[0m, in \u001b[0;36m_merge_blocks\u001b[1;34m(blocks, dtype, can_consolidate)\u001b[0m\n\u001b[0;32m   2296\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m bvals2[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_concat_same_type(bvals2, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   2298\u001b[0m argsort \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(new_mgr_locs)\n\u001b[1;32m-> 2299\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43mnew_values\u001b[49m\u001b[43m[\u001b[49m\u001b[43margsort\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   2300\u001b[0m new_mgr_locs \u001b[38;5;241m=\u001b[39m new_mgr_locs[argsort]\n\u001b[0;32m   2302\u001b[0m bp \u001b[38;5;241m=\u001b[39m BlockPlacement(new_mgr_locs)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.63 GiB for an array with shape (1510, 145302) and data type int64"
     ]
    }
   ],
   "source": [
    "columns_to_drop = [\"gameMode\", \"gameName\", \"gameType\", \"gameVersion\", \"mapId\" ]\n",
    "data_cleaned.drop(columns=columns_to_drop, inplace=True)\n",
    "data_cleaned.drop(columns= [\"p{}_summonerId\".format(i) for i in range(1, 11)], inplace=True)\n",
    "data_cleaned.drop(columns= [\"p{}_championName\".format(i) for i in range(1, 11)], inplace=True)\n",
    "data_cleaned.drop(columns= [\"p{}_individualPosition\".format(i) for i in range(1, 11)], inplace=True)\n",
    "\n",
    "columns_to_drop = [\"p{}_win\".format(i) for i in range(1, 11)]\n",
    "data_cleaned.drop(columns=[\"p{}_win\".format(i) for i in range(1,11)], inplace=True)\n",
    "data_cleaned.drop(columns=[\"p{}_nexusLost\".format(i) for i in range(1,11)], inplace=True)\n",
    "data_cleaned.drop(columns=[\"p{}_nexusKills\".format(i) for i in range(1,11)], inplace=True)\n",
    "\n",
    "data_cleaned.drop(columns= [\"p{}_summonerName\".format(i) for i in range(1, 11)], inplace=True)\n",
    "data_cleaned.drop(columns= [\"p{}_teamPosition\".format(i) for i in range(1, 11)], inplace=True)\n",
    "data_cleaned.drop(columns= [\"p{}_riotIdTagline\".format(i) for i in range(1, 11)], inplace=True)\n",
    "data_cleaned.drop(columns= [\"tournamentCode\"], inplace=True)\n",
    "data_cleaned.drop(columns= [\"p{}_puuid\".format(i) for i in range(1, 11)], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1626"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_cleaned.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145302\n"
     ]
    }
   ],
   "source": [
    "#reset index\n",
    "data_cleaned.reset_index(drop=True, inplace=True)\n",
    "print(len(data_cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'timestamps_100p' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m timestamps_100p\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m timestamps_80p\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m timestamps_60p\n",
      "\u001b[1;31mNameError\u001b[0m: name 'timestamps_100p' is not defined"
     ]
    }
   ],
   "source": [
    "del timestamps_100p\n",
    "del timestamps_80p\n",
    "del timestamps_60p\n",
    "del timestamps_40p\n",
    "del timestamps_20p\n",
    "del timestamps_starting\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matchId                       0\n",
      "p9_objectivesStolenAssists    0\n",
      "p9_onMyWayPings               0\n",
      "p8_onMyWayPings               0\n",
      "p7_onMyWayPings               0\n",
      "                             ..\n",
      "p8_bountyLevel                0\n",
      "p7_bountyLevel                0\n",
      "p6_bountyLevel                0\n",
      "p5_bountyLevel                0\n",
      "t1_objectives_tower_kills     0\n",
      "Length: 1626, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#show how many Na values are in each column and order them by the number of Na values\n",
    "na_values = data_cleaned.isna().sum().sort_values(ascending=False)\n",
    "print(na_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#onehot platformId\n",
    "data_cleaned = pd.get_dummies(data_cleaned, columns=[\"platformId\"], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Model: {model.__class__.__name__}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "\n",
    "models = [\n",
    "    LogisticRegression(random_state=42, n_jobs=-1),\n",
    "    DecisionTreeClassifier(random_state=42),\n",
    "    RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    HistGradientBoostingClassifier(random_state=42),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "def cross_val_model(model, X, y):\n",
    "    # Perform cross-validation and get accuracy scores\n",
    "    scores = cross_val_score(model, X, y, cv=5, n_jobs=-1, scoring=\"accuracy\")\n",
    "    # Print the model's class name\n",
    "    print(f\"Model: {model.__class__.__name__}\")\n",
    "    # Print all cross-validation scores\n",
    "    print(f\"Cross-validation scores (Accuracy): {scores}\")\n",
    "    # Print the mean cross-validation score\n",
    "    print(f\"Mean cross-validation score (Accuracy): {scores.mean():.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_timestamp(timestamp):\n",
    "    X, y, name = timestamp\n",
    "    print(\"timestamp at {} percent\".format(name))\n",
    "    for model in models:\n",
    "        evaluate_model(model, X, y)\n",
    "\n",
    "    # uncomment to also test with cross_validation - takes more time and results are similar so I left it commented out for now\n",
    "    # for model in models:\n",
    "    #     cross_val_model(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAKING ONEHOT ENCODING OF p_championId_x is broken a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "champion_id_frame = data_cleaned[[\"p{}_championId_x\".format(i) for i in range(1, 11)]]\n",
    "\n",
    "#it would be cool to make from 1-5 one set of onehot encoding ex. blue_championId_1 ... blue_championId_900 \n",
    "#and from 6-10 another set of onehot encoding ex. red_championId_1 ... red_championId_900\n",
    "\n",
    "#for now below attempt created that there are multiple same name collumns ex. blue_100 blue_100 ... {5 times like for each player}.\n",
    "# Combining it into one collumn after is time consuming and not sure if it even is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise NotImplementedError(\"below does not work\")\n",
    "#generate onehot encoding for championId that if championId occurs for any player in blue it will be 1 \n",
    "# and another set of columns for red team\n",
    "#print generated columns\n",
    "data_cleaned =pd.get_dummies(data_cleaned, columns=[\"p{}_championId_x\".format(i) for i in range(1,6)], prefix=\"blue\", drop_first=True)\n",
    "data_cleaned =pd.get_dummies(data_cleaned, columns=[\"p{}_championId_x\".format(i) for i in range(6,11)], prefix=\"red\", drop_first=True)\n",
    "\n",
    "#there are now multiple columns with the same name, we need to merge them\n",
    "#merge columns with same name as max() of the two\n",
    "blue_columns = [col for col in data_cleaned.columns if col.startswith(\"blue_\")]\n",
    "for col in blue_columns:\n",
    "    data_cleaned[col] = data_cleaned[blue_columns].max(axis=1)\n",
    "    \n",
    "red_columns = [col for col in data_cleaned.columns if col.startswith(\"red_\")]\n",
    "for col in red_columns:\n",
    "    data_cleaned[col] = data_cleaned[red_columns].max(axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    LogisticRegression(random_state=42, n_jobs=-1),\n",
    "    DecisionTreeClassifier(random_state=42),\n",
    "    RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    HistGradientBoostingClassifier(random_state=42),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "timestamps_starting = data_cleaned[data_cleaned[\"timeStamp\"] == 0]\n",
    "timestamps_20p = data_cleaned.loc[\n",
    "    data_cleaned.groupby(\"matchId\")[\"percentTimeStamp\"].apply(\n",
    "        lambda x: (x - 0.2).abs().idxmin()\n",
    "    )\n",
    "]\n",
    "timestamps_40p = data_cleaned.loc[\n",
    "    data_cleaned.groupby(\"matchId\")[\"percentTimeStamp\"].apply(\n",
    "        lambda x: (x - 0.4).abs().idxmin()\n",
    "    )\n",
    "]\n",
    "timestamps_60p = data_cleaned.loc[\n",
    "    data_cleaned.groupby(\"matchId\")[\"percentTimeStamp\"].apply(\n",
    "        lambda x: (x - 0.6).abs().idxmin()\n",
    "    )\n",
    "]\n",
    "timestamps_80p = data_cleaned.loc[\n",
    "    data_cleaned.groupby(\"matchId\")[\"percentTimeStamp\"].apply(\n",
    "        lambda x: (x - 0.8).abs().idxmin()\n",
    "    )\n",
    "]\n",
    "timestamps_100p = data_cleaned.loc[\n",
    "    data_cleaned.groupby(\"matchId\")[\"percentTimeStamp\"].apply(\n",
    "        lambda x: (x - 1.0).abs().idxmin()\n",
    "    )\n",
    "]\n",
    "others = [\"matchId\"]\n",
    "\n",
    "timestamps = []\n",
    "X_0 = timestamps_starting.drop(columns=[\"first_win\"] + others )\n",
    "y_0 = timestamps_starting[\"first_win\"]\n",
    "timestamps.append((X_0, y_0, \"0\"))\n",
    "\n",
    "X_20 = timestamps_20p.drop(columns=[\"first_win\"] + others)\n",
    "y_20 = timestamps_20p[\"first_win\"]\n",
    "timestamps.append((X_20, y_20, \"20\"))\n",
    "\n",
    "X_40 = timestamps_40p.drop(columns=[\"first_win\"] + others)\n",
    "y_40 = timestamps_40p[\"first_win\"]\n",
    "timestamps.append((X_40, y_40, \"40\"))\n",
    "\n",
    "X_60 = timestamps_60p.drop(columns=[\"first_win\"] + others)\n",
    "y_60 = timestamps_60p[\"first_win\"]\n",
    "timestamps.append((X_60, y_60, \"60\"))\n",
    "\n",
    "X_80 = timestamps_80p.drop(columns=[\"first_win\"] + others)\n",
    "y_80 = timestamps_80p[\"first_win\"]\n",
    "timestamps.append((X_80, y_80, \"80\"))\n",
    "\n",
    "X_100 = timestamps_100p.drop(columns=[\"first_win\"] + others)\n",
    "y_100 = timestamps_100p[\"first_win\"]\n",
    "timestamps.append((X_100, y_100, \"100\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_correlation_features(X,Y):\n",
    "    correlations = X.corrwith(Y).sort_values(ascending=False)\n",
    "\n",
    "    print(\"\\nTop 10 features most positively correlated with 'first_win':\")\n",
    "    print(correlations.head(10))\n",
    "\n",
    "    print(\"\\nTop 10 features most negatively correlated with 'first_win':\")\n",
    "    print(correlations.tail(10))\n",
    "    \n",
    "for ts, X, Y  in enumerate([(X_0, y_0), (X_20, y_20), (X_40, y_40), (X_60, y_60), (X_80, y_80), (X_100, y_100)]):\n",
    "    print(\"Timestamp: \", ts*20)\n",
    "    print_correlation_features(X, y_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    test_timestamp(timestamps[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
